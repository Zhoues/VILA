import json

data_path_list = [
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/OpenImage/filter/train_20250307_211637_015_573_filter/osd_choice_qa.json",
    # "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/OpenImage/filter/train_20250307_211637_015_573_filter/osd_reasoning_qa.json",
    # "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/OpenImage/filter/train_20250307_211637_015_573_filter/osd_template_qa.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/OpenImage/filter/train_20250307_211637_015_573_filter/osd_reasoning_template_qa.json",
    # "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/OpenImage/filter/train_20250307_211637_015_573_filter/osd_reasoning_template_qa_split.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/3D/cubifyanything/ca1m_reasoning_template_qa_split.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/3D/cubifyanything/ca1m_choice_qa_split.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/3D/cubifyanything/ca1m_visual_choice_qa.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/3D/cubifyanything/ca1m_vacant_qa.json",

    # "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/Simulator/metadata_split_10.json",

    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/Detection/refcoco/metadata.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/Detection/refcocop/metadata.json",
    "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/Detection/refcocog/metadata.json",
    # "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/vlm/SAT/metadata.json",
    # "/share/project/emllm_mnt.1d/sfs/baaiei/zhouenshen/dataset/vlm/Pixmo/pixmo_0_10_points_w_counting.json",
]

# å…¨å±€ç»Ÿè®¡å˜é‡
global_total_metadata = 0
global_total_qa = 0

# éå†æ¯ä¸ªæ–‡ä»¶
for file_path in data_path_list:
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)

        total_metadata = len(data)
        qa_counts = []

        for item in data:
            conversations = item.get("conversations", [])
            qa_count = len(conversations) // 2  # æ¯ä¸¤ä¸ªæ„æˆä¸€ä¸ª QA å¯¹
            qa_counts.append(qa_count)

        total_qa = sum(qa_counts)
        average_qa = total_qa / total_metadata if total_metadata > 0 else 0
        max_qa = max(qa_counts) if qa_counts else 0
        min_qa = min(qa_counts) if qa_counts else 0

        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        global_total_metadata += total_metadata
        global_total_qa += total_qa

        # è¾“å‡ºæ¯ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“‚ æ–‡ä»¶å: {file_path}")
        print(f"    âœ… æ€»å…±æœ‰ {total_metadata} æ¡ metadata")
        print(f"    ğŸ’¬ æ€»å…±æœ‰ {total_qa} ä¸ª QA å¯¹")
        print(f"    ğŸ“Š æ¯æ¡ metadata å¹³å‡æœ‰ {average_qa:.2f} ä¸ª QA å¯¹")
        print(f"    ğŸ”º æœ€å¤š QA æ•°é‡: {max_qa}")
        print(f"    ğŸ”» æœ€å°‘ QA æ•°é‡: {min_qa}")

    except Exception as e:
        print(f"\nâŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")

# è¾“å‡ºå…¨å±€ç»Ÿè®¡ä¿¡æ¯
global_average_qa = global_total_qa / global_total_metadata if global_total_metadata > 0 else 0

print("\nğŸ“ŠğŸ“ˆ å…¨éƒ¨æ–‡ä»¶ç»Ÿè®¡æ±‡æ€»")
print(f"    âœ… æ€» metadata æ•°é‡: {global_total_metadata}")
print(f"    ğŸ’¬ æ€» QA å¯¹æ•°é‡: {global_total_qa}")
print(f"    ğŸ“‰ å¹³å‡æ¯æ¡ metadata çš„ QA æ•°: {global_average_qa:.2f}")